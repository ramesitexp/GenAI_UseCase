{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTUbnfwNa8US40zqMh5VDm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramesitexp/genai_usecase/blob/main/local_chatbot_using_HuggingfaceMistralModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "langchain==0.0.225\n",
        "ctransformers==0.2.5\n",
        "sentence-transformers==2.2.2\n",
        "pypdf\n",
        "PyPDF2\n",
        "flask\n",
        "faiss-cpu\n",
        "-e .\n",
        "python-dotenv\n",
        "streamlit\n",
        "unstructured\n",
        "langchain_community\n",
        "langchain-text-splitters\n",
        "accelerate==0.20.3\n",
        "selenium\n",
        "playwright\n",
        "google-generativeai\n",
        "markdown\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_UEdTdg7aku",
        "outputId": "1f369a96-6e46-447c-81d2-3745d120af46"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain==0.0.225\n",
        "!pip install -q ctransformers==0.2.5\n",
        "!pip install -q sentence-transformers==2.2.2\n",
        "!pip install -q pypdf\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q flask\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q -e .\n",
        "!pip install -q streamlit\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q unstructured\n",
        "!pip install -q langchain_community\n",
        "!pip install -q langchain-text-splitters\n",
        "!pip install -q accelerate==0.20.3\n",
        "!pip install -q selenium\n",
        "!pip install -q playwright\n",
        "!pip install -q markdown\n",
        "!pip install -q google-generativeai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84_1ynHn3R9U",
        "outputId": "eaf38b8b-26c5-432f-8580-44cb6af04446"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV0l_Z8W9fOX",
        "outputId": "ed88b34e-e64e-42fa-e504-c0326d3b40c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.204.184.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGINGFACE_TOKEN=userdata.get('HUGGINGFACE_TOKEN')\n",
        "import os\n",
        "os.environ['HF_API_KEY']=HUGGINGFACE_TOKEN"
      ],
      "metadata": {
        "id": "cEgzn-Lh_bmH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUVTBBNU1l2U",
        "outputId": "344da413-902c-4eb8-c3a8-bbac57966cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import CTransformers\n",
        "# from src.helper import *\n",
        "\n",
        "import streamlit as st\n",
        "from io import StringIO\n",
        "import os\n",
        "import time\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader, MergedDataLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
        "from langchain_community.document_loaders import UnstructuredFileLoader\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pickle\n",
        "from langchain import HuggingFaceHub\n",
        "from transformers import  AutoTokenizer\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "\n",
        "\n",
        "# vector_store_file = \"./model/saved/faiss_vector_store.pkl\"\n",
        "vector_db_directory = \"./model/vectordb\"\n",
        "# llm_model_pkl_file = \"./model/saved/llama-2-7b-chat.pkl\"\n",
        "# huggingface_embedding_pkl_file = \"./model/saved/huggingface_embedding.pkl\"\n",
        "\n",
        "chain = None\n",
        "vector_db = None\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "#HF_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n",
        "#HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "\n",
        "\n",
        "if \"processComplete\" not in st.session_state:\n",
        "    st.session_state.processComplete = None\n",
        "if \"new_data_source\" not in st.session_state:\n",
        "    st.session_state.new_data_source = False\n",
        "if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = None\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = None\n",
        "\n",
        "if \"data_chunks\" not in st.session_state:\n",
        "    st.session_state.data_chunks = None\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# file process button state\n",
        "if \"file_process\" not in st.session_state:\n",
        "    st.session_state.file_process = None\n",
        "\n",
        "# Initialize chat history\n",
        "if \"upload_files\" not in st.session_state:\n",
        "    st.session_state.upload_files = None\n",
        "\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "<style>\n",
        "    .st-emotion-cache-4oy321 {\n",
        "        flex-direction: row-reverse;\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "\n",
        "#########################################################################################################\n",
        "\n",
        "def load_data_source(loaded_files):\n",
        "    for loaded_file in loaded_files:\n",
        "        print('loaded_file - ', loaded_file)\n",
        "        temp_file = create_temp_file(loaded_file)\n",
        "        # temp_file = './tmp/74862151_1709607183894.pdf'\n",
        "\n",
        "        # loader = PyPDFLoader(temp_file)\n",
        "        loader = get_loader_by_file_extension(temp_file)\n",
        "        print('loader - ', loader)\n",
        "        data = loader.load()\n",
        "        return data\n",
        "\n",
        "def get_loader_by_file_extension(temp_file):\n",
        "    file_split = os.path.splitext(temp_file)\n",
        "    file_name = file_split[0]\n",
        "    file_extension = file_split[1]\n",
        "    print('file_extension - ', file_extension)\n",
        "\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        loader = PyPDFLoader(temp_file)\n",
        "        print('Loader Created for PDF file')\n",
        "\n",
        "    elif file_extension == '.txt':\n",
        "        loader = TextLoader(temp_file)\n",
        "\n",
        "    elif file_extension == '.md':\n",
        "        loader = UnstructuredMarkdownLoader(temp_file,mode=\"elements\", strategy=\"fast\")\n",
        "\n",
        "    elif file_extension == '.csv':\n",
        "        loader = CSVLoader(temp_file)\n",
        "\n",
        "    else :\n",
        "        loader = UnstructuredFileLoader(temp_file)\n",
        "\n",
        "    return loader\n",
        "\n",
        "def create_temp_file(loaded_file):\n",
        "    # save the file temporarily\n",
        "    temp_file = f\"./tmp/{loaded_file.name}\"\n",
        "    with open(temp_file, \"wb\") as file:\n",
        "        file.write(loaded_file.getvalue())\n",
        "\n",
        "    return temp_file\n",
        "\n",
        "\n",
        "\n",
        "def get_data_chunks(data):\n",
        "    recursive_char_text_splitter=RecursiveCharacterTextSplitter(\n",
        "                                                chunk_size=500,\n",
        "                                                chunk_overlap=50)\n",
        "    documents=recursive_char_text_splitter.split_documents(data)\n",
        "    # print('documents - ', documents)\n",
        "    print('documents type - ', type(documents))\n",
        "    print('documents length - ', len(documents))\n",
        "    return documents\n",
        "\n",
        "\n",
        "def save_model(model_path, model):\n",
        "    # Save the FAISS index to a pickle file\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "\n",
        "def load_model(model_path):\n",
        "    if os.path.exists(model_path):\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            model = pickle.load(f)\n",
        "            return model\n",
        "\n",
        "\n",
        "\n",
        "def create_embeddings():\n",
        "    embeddings=HuggingFaceEmbeddings(\n",
        "            model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "            model_kwargs={'device':'cpu'}\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "\n",
        "def store_data_in_vectordb(documents, embeddings):\n",
        "    try:\n",
        "        current_vectordb = load_vectordb(vector_db_directory, embeddings)\n",
        "        print('current_vectordb - ', current_vectordb)\n",
        "    except:\n",
        "        print('Exception inside storing data in vector db')\n",
        "\n",
        "    new_knowledge_base =FAISS.from_documents(documents, embeddings)\n",
        "    print('new_knowledge_base - ', new_knowledge_base)\n",
        "\n",
        "    # Saving the new vector DB\n",
        "    new_knowledge_base.save_local(vector_db_directory)\n",
        "    return new_knowledge_base\n",
        "\n",
        "    ## TODO\n",
        "    # Adding new data to existing vector DB\n",
        "#     updated_knowledge_base = new_knowledge_base.merge_from(current_vectordb)\n",
        "#     print('updated_knowledge_base - ', updated_knowledge_base)\n",
        "\n",
        "    # Saving the new vector DB\n",
        "#     updated_knowledge_base.save_local(vector_db_directory)\n",
        "#     return updated_knowledge_base\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_vectordb(stored_directory, embeddings):\n",
        "    loaded_vector_db = FAISS.load_local(stored_directory, embeddings)\n",
        "    return loaded_vector_db\n",
        "\n",
        "\n",
        "def get_llm_model():\n",
        "    # llm=CTransformers(\n",
        "    #         model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
        "    #         model_type=\"llama\",\n",
        "    #         config={'max_new_tokens':128,\n",
        "    #                 'temperature':0.01}\n",
        "    # )\n",
        "\n",
        "    # llm = AutoModelForCausalLM.from_pretrained(\"./model/mistral-7b-instruct-v0.1.Q4_K_S.gguf\", model_type=\"cpu\")\n",
        "    llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", huggingfacehub_api_token=\"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\")\n",
        "    print('LLM model Loaded')\n",
        "    return llm\n",
        "\n",
        "\n",
        "\n",
        "def get_prompt():\n",
        "    template=\"\"\"Use the following pieces of information to answer the user's question.\n",
        "            If you dont know the answer just say you dont know, don't try to make up an answer.\n",
        "\n",
        "            Context:{context}\n",
        "            Question:{question}\n",
        "\n",
        "            Only return the helpful answer below and nothing else\n",
        "            Helpful answer\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "    print('Prompt created')\n",
        "    return prompt\n",
        "\n",
        "def create_chain(llm, vector_store, prompt):\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type='stuff',\n",
        "            retriever=vector_store.as_retriever(search_kwargs={'k': 2}),\n",
        "            return_source_documents=False,\n",
        "            chain_type_kwargs={'prompt': prompt}\n",
        "    )\n",
        "    print('Chain created')\n",
        "    return chain\n",
        "\n",
        "def create_conversational_chain(llm, vector_store, prompt):\n",
        "    memory = ConversationBufferMemory()\n",
        "    conversation_chain = ConversationalRetrievalChain(\n",
        "        llm=llm,\n",
        "        chain_type='stuff',\n",
        "        retriever=vector_store.as_retriever(search_kwargs={'k': 2}),\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={'prompt': prompt},\n",
        "        memory=memory\n",
        "    )\n",
        "    print('Chain created')\n",
        "    return conversation_chain\n",
        "\n",
        "def get_similiar_docs(query,k=1,score=False):\n",
        "  if score:\n",
        "    similar_docs = vector_db.similarity_search_with_score(query,k=k)\n",
        "  else:\n",
        "    similar_docs = vector_db.similarity_search(query,k=k)\n",
        "  return similar_docs\n",
        "\n",
        "\n",
        "def process_for_new_data_source(uploaded_files):\n",
        "\n",
        "        with st.spinner('Processing, Wait for it...'):\n",
        "\n",
        "                # #Load the PDF File\n",
        "                documents = load_data_source(uploaded_files)\n",
        "\n",
        "                # #Split Text into Chunks\n",
        "                st.session_state.data_chunks = get_data_chunks(documents)\n",
        "\n",
        "                # #Load the Embedding Model\n",
        "                embeddings = create_embeddings()\n",
        "\n",
        "                # #Convert the Text Chunks into Embeddings and Create a FAISS Vector Store\n",
        "                vector_db=store_data_in_vectordb(st.session_state.data_chunks, embeddings)\n",
        "\n",
        "                llm = get_llm_model()\n",
        "\n",
        "                qa_prompt = get_prompt()\n",
        "\n",
        "                chain = create_chain(llm, vector_db, qa_prompt)\n",
        "\n",
        "                st.text(\"Ready to go ...✅✅✅\")\n",
        "                st.session_state.processComplete = True\n",
        "\n",
        "                return chain\n",
        "\n",
        "\n",
        "def process_for_existing_source():\n",
        "\n",
        "    st.session_state.conversation = True\n",
        "    # #Load the Embedding Model\n",
        "    embeddings = create_embeddings()\n",
        "    vector_db = load_vectordb(vector_db_directory, embeddings)\n",
        "    llm = get_llm_model()\n",
        "    qa_prompt = get_prompt()\n",
        "    chain = create_chain(llm, vector_db, qa_prompt)\n",
        "    return chain\n",
        "\n",
        "\n",
        "def get_response(user_query):\n",
        "#     chain = process_for_new_data_source()\n",
        "#     user_input = \"Tell me about transformers in NLP\"\n",
        "#     user_input = \"what is capital of india\"\n",
        "#     user_input = \"What is the account no of Sagar\"\n",
        "\n",
        "#     similarity_search_value = vector_db.similarity_search(user_input)\n",
        "#     print('similarity_search_value - ', similarity_search_value)\n",
        "\n",
        "#     print('vector_db - ', vector_db)\n",
        "\n",
        "    print('user_query - ', user_query)\n",
        "    # print('Inside get_response  -', st.session_state.conversation)\n",
        "\n",
        "    if st.session_state.conversation :\n",
        "\n",
        "        result=st.session_state.conversation({'query':user_query}, return_only_outputs=True)\n",
        "        print('result - ', result)\n",
        "        ans = result['result']\n",
        "        print(f\"Answer:{ans}\")\n",
        "        return ans\n",
        "\n",
        "\n",
        "def sidebar_design():\n",
        "    with st.sidebar:\n",
        "        st.title(\"Data sources\")\n",
        "        st.session_state.upload_files =  st.file_uploader(\n",
        "            \"Upload your file\",\n",
        "            type=['pdf','txt', 'csv', 'doc/docx'],\n",
        "            accept_multiple_files=True\n",
        "        )\n",
        "        URL_TO_EXTRACT = st.text_input(\"Site URL\")\n",
        "        st.session_state.file_process = st.button(\"Process\")\n",
        "\n",
        "\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "\n",
        "# def main2():\n",
        "#     st.subheader(\"Chat With Your Documents\")\n",
        "#     user_question = st.chat_input(\"Ask Question about your files.\")\n",
        "\n",
        "\n",
        "#     with st.sidebar:\n",
        "#         st.title(\"Data sources\")\n",
        "#         uploaded_files =  st.file_uploader(\n",
        "#             \"Upload your file\",\n",
        "#             type=['pdf','txt', 'csv', 'doc/docx'],\n",
        "#             accept_multiple_files=True\n",
        "#         )\n",
        "#         URL_TO_EXTRACT = st.text_input(\"Site URL\")\n",
        "#         process = st.button(\"Process\")\n",
        "\n",
        "#         if process:\n",
        "#             if uploaded_files:\n",
        "#                 st.session_state.new_data_source = True\n",
        "#                 st.session_state.conversation = process_for_new_data_source(uploaded_files)\n",
        "\n",
        "\n",
        "#             elif URL_TO_EXTRACT:\n",
        "#                 st.session_state.new_data_source = True\n",
        "\n",
        "#             else:\n",
        "#                 st.session_state.new_data_source = False\n",
        "\n",
        "#         else:\n",
        "#             st.session_state.processComplete = True\n",
        "\n",
        "#     if st.session_state.processComplete == True :\n",
        "\n",
        "\n",
        "\n",
        "#         with st.chat_message(\"assistant\"):\n",
        "#             st.write(\"Hello Human\")\n",
        "\n",
        "#             if user_question :\n",
        "#                     # user_input = \"What is the account no of Sagar\"\n",
        "#                     st.text(f'You : {user_question}', )\n",
        "#                     response = get_response(user_question)\n",
        "#                     st.text(f'Bot : {response}', )\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.title(\"Chat with Your documents !!\")\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.title(\"Data sources\")\n",
        "        st.session_state.upload_files =  st.file_uploader(\n",
        "            \"Upload your file\",\n",
        "            type=['pdf','txt', 'csv','md','doc/docx'],\n",
        "            accept_multiple_files=True\n",
        "        )\n",
        "        URL_TO_EXTRACT = st.text_input(\"Site URL\")\n",
        "        st.session_state.file_process = st.button(\"Process\")\n",
        "\n",
        "        if st.session_state.file_process:\n",
        "            if st.session_state.upload_files:\n",
        "                st.session_state.new_data_source = True\n",
        "                st.session_state.conversation = process_for_new_data_source(st.session_state.upload_files)\n",
        "\n",
        "            elif URL_TO_EXTRACT:\n",
        "                st.session_state.new_data_source = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.write(\"Hello Human, How do I help U.\")\n",
        "\n",
        "    # Display chat messages from history on app rerun\n",
        "    for message in st.session_state.messages:\n",
        "        print('message - ', message)\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    # React to user input\n",
        "    if prompt := st.chat_input(\"Ask Question about your files.\"):\n",
        "        # Display user message in chat message container\n",
        "        st.chat_message(\"user\").markdown(prompt)\n",
        "        # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        print('st.session_state.new_data_source ---> ', st.session_state.new_data_source)\n",
        "        if st.session_state.new_data_source == False:\n",
        "            process_for_existing_source()\n",
        "\n",
        "\n",
        "        # Display assistant response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            print('Inside st.chat_message(\"assistant\")')\n",
        "            with st.spinner('Processing ...'):\n",
        "                response = get_response(prompt)\n",
        "                st.markdown(response)\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "O9r1HsUE3HN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0b62e9-0771-416a-e977-e82cdbbc15dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.204.184.82:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.34s\n",
            "your url is: https://tricky-spoons-shake.loca.lt\n",
            "loaded_file -  UploadedFile(file_id='2d162cc5-22a0-4087-ae34-2bad171852e0', name='alice_in_wonderland.md', type='application/octet-stream', size=170002, _file_urls=file_id: \"2d162cc5-22a0-4087-ae34-2bad171852e0\"\n",
            "upload_url: \"/_stcore/upload_file/1f2860f5-dee5-4862-8a82-470b9155c87f/2d162cc5-22a0-4087-ae34-2bad171852e0\"\n",
            "delete_url: \"/_stcore/upload_file/1f2860f5-dee5-4862-8a82-470b9155c87f/2d162cc5-22a0-4087-ae34-2bad171852e0\"\n",
            ")\n",
            "file_extension -  .md\n",
            "loader -  <langchain.document_loaders.markdown.UnstructuredMarkdownLoader object at 0x7a89651f1f00>\n",
            "documents type -  <class 'list'>\n",
            "documents length -  929\n",
            "current_vectordb -  <langchain.vectorstores.faiss.FAISS object at 0x7a892ee17280>\n",
            "new_knowledge_base -  <langchain.vectorstores.faiss.FAISS object at 0x7a892ee17370>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "LLM model Loaded\n",
            "Prompt created\n",
            "Chain created\n",
            "st.session_state.new_data_source --->  True\n",
            "Inside st.chat_message(\"assistant\")\n",
            "user_query -  Hello\n",
            "result -  {'result': \"------------------------------\\n            Hello! How may I assist you with your query today regarding CHORUS? We'd be happy to help answer any questions you might have. Could you please provide more details about what specifically you'd like to know? For instance, you might be asking about CHORUS authentication, access, submission, or any other related topic. Please let us know and we'll do our best to help provide accurate and up-to-date information. Additionally, feel free\"}\n",
            "Answer:------------------------------\n",
            "            Hello! How may I assist you with your query today regarding CHORUS? We'd be happy to help answer any questions you might have. Could you please provide more details about what specifically you'd like to know? For instance, you might be asking about CHORUS authentication, access, submission, or any other related topic. Please let us know and we'll do our best to help provide accurate and up-to-date information. Additionally, feel free\n",
            "message -  {'role': 'user', 'content': 'Hello'}\n",
            "message -  {'role': 'assistant', 'content': \"------------------------------\\n            Hello! How may I assist you with your query today regarding CHORUS? We'd be happy to help answer any questions you might have. Could you please provide more details about what specifically you'd like to know? For instance, you might be asking about CHORUS authentication, access, submission, or any other related topic. Please let us know and we'll do our best to help provide accurate and up-to-date information. Additionally, feel free\"}\n",
            "st.session_state.new_data_source --->  True\n",
            "Inside st.chat_message(\"assistant\")\n",
            "user_query -  Who is Alice?\n",
            "result -  {'result': ' Alice is the person whom everybody was looking at in the context.'}\n",
            "Answer: Alice is the person whom everybody was looking at in the context.\n",
            "message -  {'role': 'user', 'content': 'Hello'}\n",
            "message -  {'role': 'assistant', 'content': \"------------------------------\\n            Hello! How may I assist you with your query today regarding CHORUS? We'd be happy to help answer any questions you might have. Could you please provide more details about what specifically you'd like to know? For instance, you might be asking about CHORUS authentication, access, submission, or any other related topic. Please let us know and we'll do our best to help provide accurate and up-to-date information. Additionally, feel free\"}\n",
            "message -  {'role': 'user', 'content': 'Who is Alice?'}\n",
            "message -  {'role': 'assistant', 'content': ' Alice is the person whom everybody was looking at in the context.'}\n",
            "st.session_state.new_data_source --->  True\n",
            "Inside st.chat_message(\"assistant\")\n",
            "user_query -  Please  provide me the summary of the story\n",
            "result -  {'result': '-------------------\\n\\n    In the beginning of the story, Alice falls into a rabbit hole and journeys through a fantastical world filled with peculiar creatures and strange occurrences. At one point, she encounters a pack of playing cards that chase her, but she manages to escape and finds herself lying on the bank with her sister, who calms her down. The events described in this excerpt occur after Alice has fallen down the rabbit hole but before she meets the'}\n",
            "Answer:-------------------\n",
            "\n",
            "    In the beginning of the story, Alice falls into a rabbit hole and journeys through a fantastical world filled with peculiar creatures and strange occurrences. At one point, she encounters a pack of playing cards that chase her, but she manages to escape and finds herself lying on the bank with her sister, who calms her down. The events described in this excerpt occur after Alice has fallen down the rabbit hole but before she meets the\n",
            "message -  {'role': 'user', 'content': 'Hello'}\n",
            "message -  {'role': 'assistant', 'content': \"------------------------------\\n            Hello! How may I assist you with your query today regarding CHORUS? We'd be happy to help answer any questions you might have. Could you please provide more details about what specifically you'd like to know? For instance, you might be asking about CHORUS authentication, access, submission, or any other related topic. Please let us know and we'll do our best to help provide accurate and up-to-date information. Additionally, feel free\"}\n",
            "message -  {'role': 'user', 'content': 'Who is Alice?'}\n",
            "message -  {'role': 'assistant', 'content': ' Alice is the person whom everybody was looking at in the context.'}\n",
            "message -  {'role': 'user', 'content': 'Please  provide me the summary of the story'}\n",
            "message -  {'role': 'assistant', 'content': '-------------------\\n\\n    In the beginning of the story, Alice falls into a rabbit hole and journeys through a fantastical world filled with peculiar creatures and strange occurrences. At one point, she encounters a pack of playing cards that chase her, but she manages to escape and finds herself lying on the bank with her sister, who calms her down. The events described in this excerpt occur after Alice has fallen down the rabbit hole but before she meets the'}\n",
            "st.session_state.new_data_source --->  True\n",
            "Inside st.chat_message(\"assistant\")\n",
            "user_query -  What is the plot of the story?\n",
            "result -  {'result': \"ources for this answer can be the text itself, earlier questions and answers, or general knowledge.\\n\\nThe plot of the story follows Alice as she falls down a rabbit hole and enters a fantastical world populated by strange characters like the White Rabbit, the March Hare, the Mad Hatter, and the Queen of Hearts. Alice experiences various surreal and bizarre events as she tries to find a way back home. The details of the plot include Alice's encounter\"}\n",
            "Answer:ources for this answer can be the text itself, earlier questions and answers, or general knowledge.\n",
            "\n",
            "The plot of the story follows Alice as she falls down a rabbit hole and enters a fantastical world populated by strange characters like the White Rabbit, the March Hare, the Mad Hatter, and the Queen of Hearts. Alice experiences various surreal and bizarre events as she tries to find a way back home. The details of the plot include Alice's encounter\n",
            "message -  {'role': 'user', 'content': 'Hello'}\n",
            "message -  {'role': 'assistant', 'content': \"------------------------------\\n            Hello! How may I assist you with your query today regarding CHORUS? We'd be happy to help answer any questions you might have. Could you please provide more details about what specifically you'd like to know? For instance, you might be asking about CHORUS authentication, access, submission, or any other related topic. Please let us know and we'll do our best to help provide accurate and up-to-date information. Additionally, feel free\"}\n",
            "message -  {'role': 'user', 'content': 'Who is Alice?'}\n",
            "message -  {'role': 'assistant', 'content': ' Alice is the person whom everybody was looking at in the context.'}\n",
            "message -  {'role': 'user', 'content': 'Please  provide me the summary of the story'}\n",
            "message -  {'role': 'assistant', 'content': '-------------------\\n\\n    In the beginning of the story, Alice falls into a rabbit hole and journeys through a fantastical world filled with peculiar creatures and strange occurrences. At one point, she encounters a pack of playing cards that chase her, but she manages to escape and finds herself lying on the bank with her sister, who calms her down. The events described in this excerpt occur after Alice has fallen down the rabbit hole but before she meets the'}\n",
            "message -  {'role': 'user', 'content': 'What is the plot of the story?'}\n",
            "message -  {'role': 'assistant', 'content': \"ources for this answer can be the text itself, earlier questions and answers, or general knowledge.\\n\\nThe plot of the story follows Alice as she falls down a rabbit hole and enters a fantastical world populated by strange characters like the White Rabbit, the March Hare, the Mad Hatter, and the Queen of Hearts. Alice experiences various surreal and bizarre events as she tries to find a way back home. The details of the plot include Alice's encounter\"}\n",
            "st.session_state.new_data_source --->  True\n",
            "Inside st.chat_message(\"assistant\")\n",
            "user_query -  what is the summary of the story?\n",
            "result -  {'result': ' Alice falls down a rabbit hole, encounters strange creatures, and begins her journey in Wonderland.\\n             In this scene from \"Alice\\'s Adventures in Wonderland,\" Alice is invited to tell a story by the March Hare and the Mad Hatter, but she doesn\\'t know where to begin. Instead, she finds herself falling down a rabbit hole, which leads her to an extraordinary world filled with peculiar creatures like the White Rabbit, the Caterpill'}\n",
            "Answer: Alice falls down a rabbit hole, encounters strange creatures, and begins her journey in Wonderland.\n",
            "             In this scene from \"Alice's Adventures in Wonderland,\" Alice is invited to tell a story by the March Hare and the Mad Hatter, but she doesn't know where to begin. Instead, she finds herself falling down a rabbit hole, which leads her to an extraordinary world filled with peculiar creatures like the White Rabbit, the Caterpill\n",
            "message -  {'role': 'user', 'content': 'Hello'}\n",
            "message -  {'role': 'assistant', 'content': \"------------------------------\\n            Hello! How may I assist you with your query today regarding CHORUS? We'd be happy to help answer any questions you might have. Could you please provide more details about what specifically you'd like to know? For instance, you might be asking about CHORUS authentication, access, submission, or any other related topic. Please let us know and we'll do our best to help provide accurate and up-to-date information. Additionally, feel free\"}\n",
            "message -  {'role': 'user', 'content': 'Who is Alice?'}\n",
            "message -  {'role': 'assistant', 'content': ' Alice is the person whom everybody was looking at in the context.'}\n",
            "message -  {'role': 'user', 'content': 'Please  provide me the summary of the story'}\n",
            "message -  {'role': 'assistant', 'content': '-------------------\\n\\n    In the beginning of the story, Alice falls into a rabbit hole and journeys through a fantastical world filled with peculiar creatures and strange occurrences. At one point, she encounters a pack of playing cards that chase her, but she manages to escape and finds herself lying on the bank with her sister, who calms her down. The events described in this excerpt occur after Alice has fallen down the rabbit hole but before she meets the'}\n",
            "message -  {'role': 'user', 'content': 'What is the plot of the story?'}\n",
            "message -  {'role': 'assistant', 'content': \"ources for this answer can be the text itself, earlier questions and answers, or general knowledge.\\n\\nThe plot of the story follows Alice as she falls down a rabbit hole and enters a fantastical world populated by strange characters like the White Rabbit, the March Hare, the Mad Hatter, and the Queen of Hearts. Alice experiences various surreal and bizarre events as she tries to find a way back home. The details of the plot include Alice's encounter\"}\n",
            "message -  {'role': 'user', 'content': 'what is the summary of the story?'}\n",
            "message -  {'role': 'assistant', 'content': ' Alice falls down a rabbit hole, encounters strange creatures, and begins her journey in Wonderland.\\n             In this scene from \"Alice\\'s Adventures in Wonderland,\" Alice is invited to tell a story by the March Hare and the Mad Hatter, but she doesn\\'t know where to begin. Instead, she finds herself falling down a rabbit hole, which leads her to an extraordinary world filled with peculiar creatures like the White Rabbit, the Caterpill'}\n",
            "st.session_state.new_data_source --->  True\n",
            "Inside st.chat_message(\"assistant\")\n",
            "user_query -  what is the summary of the story?\n",
            "\n",
            "\n",
            "result -  {'result': '---------------------------------------------------\\n\\nIn \"Down the Rabbit-Hole,\" the first chapter of Lewis Carroll\\'s Alice\\'s Adventures in Wonderland, Alice follows a White Rabbit who is wearing a waistcoat and carrying a pocket watch, down a rabbit hole. She falls deep into Wonderland, encountering various strange and fantastical creatures such as the Cheshire Cat, the Mad Hatter, and the March Hare. Each character seems to be involved in'}\n",
            "Answer:---------------------------------------------------\n",
            "\n",
            "In \"Down the Rabbit-Hole,\" the first chapter of Lewis Carroll's Alice's Adventures in Wonderland, Alice follows a White Rabbit who is wearing a waistcoat and carrying a pocket watch, down a rabbit hole. She falls deep into Wonderland, encountering various strange and fantastical creatures such as the Cheshire Cat, the Mad Hatter, and the March Hare. Each character seems to be involved in\n"
          ]
        }
      ]
    }
  ]
}