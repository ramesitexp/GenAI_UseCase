{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJeTYPbdMb2IAN2B1VsIFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramesitexp/genai_usecase/blob/main/Extracting_Structure_Output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DHoJ4-qdjNSf"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q langchain-openai\n",
        "!pip install -q langchain-mistralai\n",
        "!pip install -q langchain-fireworks\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(\n",
        "        default=None, description=\"The color of the peron's hair if known\"\n",
        "    )\n",
        "    height_in_meters: Optional[str] = Field(\n",
        "        default=None, description=\"Height measured in meters\"\n",
        "    )"
      ],
      "metadata": {
        "id": "_lufSm-RkQB9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
        "#    about the document from which the text was extracted.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm. \"\n",
        "            \"Only extract relevant information from the text. \"\n",
        "            \"If you do not know the value of an attribute asked to extract, \"\n",
        "            \"return null for the attribute's value.\",\n",
        "        ),\n",
        "        # Please see the how-to about improving performance with\n",
        "        # reference examples.\n",
        "        # MessagesPlaceholder('examples'),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "4UamHmCIkSIT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
        "\n",
        "runnable = prompt | llm.with_structured_output(schema=Person)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI46yDQ0kV7U",
        "outputId": "6c79b034-7d7d-4538-fbed-bc87c56f9e14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
        "runnable.invoke({\"text\": text})"
      ],
      "metadata": {
        "id": "5kiYH7qOkd0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(\n",
        "        default=None, description=\"The color of the peron's hair if known\"\n",
        "    )\n",
        "    height_in_meters: Optional[str] = Field(\n",
        "        default=None, description=\"Height measured in meters\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about people.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple entities.\n",
        "    people: List[Person]"
      ],
      "metadata": {
        "id": "lvVdDRX_ky0G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = prompt | llm.with_structured_output(schema=Data)\n",
        "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
        "runnable.invoke({\"text\": text})"
      ],
      "metadata": {
        "id": "YmFGQR_7k0vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use reference examples"
      ],
      "metadata": {
        "id": "ZNslOPu4m_2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
        "#    about the document from which the text was extracted.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm. \"\n",
        "            \"Only extract relevant information from the text. \"\n",
        "            \"If you do not know the value of an attribute asked \"\n",
        "            \"to extract, return null for the attribute's value.\",\n",
        "        ),\n",
        "        # ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "        MessagesPlaceholder(\"examples\"),  # <-- EXAMPLES!\n",
        "        # ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "YUzQDCJnnCHL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        ")\n",
        "\n",
        "prompt.invoke(\n",
        "    {\"text\": \"this is some text\", \"examples\": [HumanMessage(content=\"testing 1 2 3\")]}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_cWUHOnnB68",
        "outputId": "ed0821b9-d1ad-4876-dfa8-640845cf7596"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\"), HumanMessage(content='testing 1 2 3'), HumanMessage(content='this is some text')])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "    name: Optional[str] = Field(..., description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(\n",
        "        ..., description=\"The color of the peron's eyes if known\"\n",
        "    )\n",
        "    height_in_meters: Optional[str] = Field(..., description=\"Height in METERs\")\n",
        "\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about people.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple entities.\n",
        "    people: List[Person]"
      ],
      "metadata": {
        "id": "48-3P9S2nHZD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from typing import Dict, List, TypedDict\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Example(TypedDict):\n",
        "    \"\"\"A representation of an example consisting of text input and expected tool calls.\n",
        "\n",
        "    For extraction, the tool calls are represented as instances of pydantic model.\n",
        "    \"\"\"\n",
        "\n",
        "    input: str  # This is the example text\n",
        "    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted\n",
        "\n",
        "\n",
        "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
        "    \"\"\"Convert an example into a list of messages that can be fed into an LLM.\n",
        "\n",
        "    This code is an adapter that converts our example to a list of messages\n",
        "    that can be fed into a chat model.\n",
        "\n",
        "    The list of messages per example corresponds to:\n",
        "\n",
        "    1) HumanMessage: contains the content from which content should be extracted.\n",
        "    2) AIMessage: contains the extracted information from the model\n",
        "    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\n",
        "\n",
        "    The ToolMessage is required because some of the chat models are hyper-optimized for agents\n",
        "    rather than for an extraction use case.\n",
        "    \"\"\"\n",
        "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
        "    openai_tool_calls = []\n",
        "    for tool_call in example[\"tool_calls\"]:\n",
        "        openai_tool_calls.append(\n",
        "            {\n",
        "                \"id\": str(uuid.uuid4()),\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    # The name of the function right now corresponds\n",
        "                    # to the name of the pydantic model\n",
        "                    # This is implicit in the API right now,\n",
        "                    # and will be improved over time.\n",
        "                    \"name\": tool_call.__class__.__name__,\n",
        "                    \"arguments\": tool_call.json(),\n",
        "                },\n",
        "            }\n",
        "        )\n",
        "    messages.append(\n",
        "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
        "    )\n",
        "    tool_outputs = example.get(\"tool_outputs\") or [\n",
        "        \"You have correctly called this tool.\"\n",
        "    ] * len(openai_tool_calls)\n",
        "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
        "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
        "    return messages"
      ],
      "metadata": {
        "id": "rC1tCPBSnTMc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    (\n",
        "        \"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\",\n",
        "        Person(name=None, height_in_meters=None, hair_color=None),\n",
        "    ),\n",
        "    (\n",
        "        \"Fiona traveled far from France to Spain.\",\n",
        "        Person(name=\"Fiona\", height_in_meters=None, hair_color=None),\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "\n",
        "for text, tool_call in examples:\n",
        "    messages.extend(\n",
        "        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})\n",
        "    )"
      ],
      "metadata": {
        "id": "BEHlEUG9nWLT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.invoke({\"text\": \"this is some text\", \"examples\": messages})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldc2z2vnnXxj",
        "outputId": "7c8dcbc3-6df1-4f6f-eb43-9041a2c70f00"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\"), HumanMessage(content=\"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\"), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '9c0f6eb7-5b14-4cc2-b88f-1cccddbc6a30', 'type': 'function', 'function': {'name': 'Person', 'arguments': '{\"name\": null, \"hair_color\": null, \"height_in_meters\": null}'}}]}, tool_calls=[{'name': 'Person', 'args': {'name': None, 'hair_color': None, 'height_in_meters': None}, 'id': '9c0f6eb7-5b14-4cc2-b88f-1cccddbc6a30'}]), ToolMessage(content='You have correctly called this tool.', tool_call_id='9c0f6eb7-5b14-4cc2-b88f-1cccddbc6a30'), HumanMessage(content='Fiona traveled far from France to Spain.'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'c334f687-6d49-4e9e-84d2-fe93b353cb4f', 'type': 'function', 'function': {'name': 'Person', 'arguments': '{\"name\": \"Fiona\", \"hair_color\": null, \"height_in_meters\": null}'}}]}, tool_calls=[{'name': 'Person', 'args': {'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}, 'id': 'c334f687-6d49-4e9e-84d2-fe93b353cb4f'}]), ToolMessage(content='You have correctly called this tool.', tool_call_id='c334f687-6d49-4e9e-84d2-fe93b353cb4f'), HumanMessage(content='this is some text')])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "6yJNdysNnqzK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be using tool calling mode, which\n",
        "# requires a tool calling capable model.\n",
        "llm = ChatOpenAI(\n",
        "    # Consider benchmarking with a good model to get\n",
        "    # a sense of the best possible quality.\n",
        "    model=\"gpt-4-0125-preview\",\n",
        "    # Remember to set the temperature to 0 for extractions!\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "\n",
        "runnable = prompt | llm.with_structured_output(\n",
        "    schema=Data,\n",
        "    method=\"function_calling\",\n",
        "    include_raw=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8veMLiIOnchE",
        "outputId": "0d05df0d-4e87-42d4-a93c-b383bfd4616a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    text = \"The solar system is large, but earth has only 1 moon.\"\n",
        "    print(runnable.invoke({\"text\": text, \"examples\": []}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akRNn2m0n5Jq",
        "outputId": "9d56a10f-dc16-43bc-901d-fe030acfff96"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "people=[]\n",
            "people=[]\n",
            "people=[]\n",
            "people=[]\n",
            "people=[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runnable.invoke(\n",
        "    {\n",
        "        \"text\": \"My name is Harrison. My hair is black.\",\n",
        "        \"examples\": messages,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESeirlATn9ta",
        "outputId": "916e297e-99db-4172-bd35-dd27c525fb56"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Harrison', hair_color='black', height_in_meters=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handle long text"
      ],
      "metadata": {
        "id": "RaCObSQQoWZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import requests\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "# Download the content\n",
        "response = requests.get(\"https://en.wikipedia.org/wiki/Car\")\n",
        "# Write it to a file\n",
        "with open(\"car.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "# Load it with an HTML parser\n",
        "loader = BSHTMLLoader(\"car.html\")\n",
        "document = loader.load()[0]\n",
        "# Clean up code\n",
        "# Replace consecutive new lines with a single new line\n",
        "document.page_content = re.sub(\"\\n\\n+\", \"\\n\", document.page_content)"
      ],
      "metadata": {
        "id": "3XGh0jZUoZDU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(document.page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG6vrciqoWEi",
        "outputId": "e60816c1-f58f-42ed-c9a1-cf38bf71e8d0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from langchain.chains import create_structured_output_runnable\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "class KeyDevelopment(BaseModel):\n",
        "    \"\"\"Information about a development in the history of cars.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "    # Note that all fields are required rather than optional!\n",
        "    year: int = Field(\n",
        "        ..., description=\"The year when there was an important historic development.\"\n",
        "    )\n",
        "    description: str = Field(\n",
        "        ..., description=\"What happened in this year? What was the development?\"\n",
        "    )\n",
        "    evidence: str = Field(\n",
        "        ...,\n",
        "        description=\"Repeat in verbatim the sentence(s) from which the year and description information were extracted\",\n",
        "    )\n",
        "\n",
        "\n",
        "class ExtractionData(BaseModel):\n",
        "    \"\"\"Extracted information about key developments in the history of cars.\"\"\"\n",
        "\n",
        "    key_developments: List[KeyDevelopment]\n",
        "\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
        "#    about the document from which the text was extracted.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert at identifying key historic development in text. \"\n",
        "            \"Only extract important historic developments. Extract nothing if no important information can be found in the text.\",\n",
        "        ),\n",
        "        # MessagesPlaceholder('examples'), # Keep on reading through this use case to see how to use examples to improve performance\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# We will be using tool calling mode, which\n",
        "# requires a tool calling capable model.\n",
        "llm = ChatOpenAI(\n",
        "    # Consider benchmarking with a good model to get\n",
        "    # a sense of the best possible quality.\n",
        "    model=\"gpt-4-0125-preview\",\n",
        "    # Remember to set the temperature to 0 for extractions!\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "extractor = prompt | llm.with_structured_output(\n",
        "    schema=ExtractionData,\n",
        "    method=\"function_calling\",\n",
        "    include_raw=False,\n",
        ")"
      ],
      "metadata": {
        "id": "mZA2Vy6xoedt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(\n",
        "    # Controls the size of each chunk\n",
        "    chunk_size=2000,\n",
        "    # Controls overlap between chunks\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(document.page_content)"
      ],
      "metadata": {
        "id": "lyYSpquUo4hz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit just to the first 3 chunks\n",
        "# so the code can be re-run quickly\n",
        "first_few = texts[:3]\n",
        "\n",
        "extractions = extractor.batch(\n",
        "    [{\"text\": text} for text in first_few],\n",
        "    {\"max_concurrency\": 5},  # limit the concurrency by passing max concurrency!\n",
        ")"
      ],
      "metadata": {
        "id": "dKzfpOlJpD1T"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_developments = []\n",
        "\n",
        "for extraction in extractions:\n",
        "    key_developments.extend(extraction.key_developments)\n",
        "\n",
        "key_developments[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djqo_g9VpIVq",
        "outputId": "136958a1-8055-4b87-a55a-03a6e7e07bea"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[KeyDevelopment(year=1966, description=\"The Toyota Corolla began production, recognized as the world's best-selling automobile.\", evidence=\"The Toyota Corolla has been in production since 1966 and is recognized as the world's best-selling automobile.\"),\n",
              " KeyDevelopment(year=1769, description='Nicolas-Joseph Cugnot built the first steam-powered road vehicle.', evidence='French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769.'),\n",
              " KeyDevelopment(year=1808, description='François Isaac de Rivaz designed and constructed the first internal combustion-powered automobile.', evidence='French-born Swiss inventor François Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'),\n",
              " KeyDevelopment(year=1886, description='Carl Benz patented his Benz Patent-Motorwagen, inventing the modern car.', evidence='The modern car—a practical, marketable automobile for everyday use—was invented in 1886, when German inventor Carl Benz patented his Benz Patent-Motorwagen.'),\n",
              " KeyDevelopment(year=1908, description='The 1908 Model T, an affordable car for the masses, was manufactured by the Ford Motor Company.', evidence='One of the first cars affordable by the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company.'),\n",
              " KeyDevelopment(year=1888, description=\"Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.\", evidence=\"In August 1888, Bertha Benz, the wife of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\"),\n",
              " KeyDevelopment(year=1896, description='Benz designed and patented the first internal-combustion flat engine, called boxermotor.', evidence='In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor.'),\n",
              " KeyDevelopment(year=1897, description='Nesselsdorfer Wagenbau produced the Präsident automobil, one of the first factory-made cars in the world.', evidence='The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Präsident automobil.'),\n",
              " KeyDevelopment(year=1890, description='Daimler Motoren Gesellschaft (DMG) was founded by Daimler and Maybach in Cannstatt.', evidence='Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt in 1890.'),\n",
              " KeyDevelopment(year=1892, description='Daimler sold their first car under the brand name Daimler.', evidence='Daimler and Maybach...sold their first car in 1892 under the brand name Daimler.'),\n",
              " KeyDevelopment(year=1902, description='A new model DMG car was produced and named Mercedes after the Maybach engine.', evidence='Two years later, in 1902, a new model DMG car was produced and the model was named Mercedes after the Maybach engine, which generated 35 hp.'),\n",
              " KeyDevelopment(year=1891, description='Auguste Doriot and Louis Rigoulot completed the longest trip by a petrol-driven vehicle in a Peugeot Type 3.', evidence='In 1891, Auguste Doriot and his Peugeot colleague Louis Rigoulot completed the longest trip by a petrol-driven vehicle when their self-designed and built Daimler powered Peugeot Type 3 completed 2,100 kilometres (1,300 mi) from Valentigney to Paris and Brest and back again.'),\n",
              " KeyDevelopment(year=1895, description='George Selden was granted a US patent for a two-stroke car engine.', evidence='After a delay of 16 years and a series of attachments to his application, on 5 November 1895, Selden was granted a US patent (U.S. patent 549,160) for a two-stroke car engine.'),\n",
              " KeyDevelopment(year=1893, description='The first running, petrol-driven American car was built and road-tested by the Duryea brothers.', evidence='In 1893, the first running, petrol-driven American car was built and road-tested by the Duryea brothers of Springfield, Massachusetts.'),\n",
              " KeyDevelopment(year=1897, description='Rudolf Diesel built the first diesel engine.', evidence='In 1897, he built the first diesel engine.'),\n",
              " KeyDevelopment(year=1901, description='Ransom Olds started large-scale, production-line manufacturing of affordable cars.', evidence='Large-scale, production-line manufacturing of affordable cars was started by Ransom Olds in 1901 at his Oldsmobile factory in Lansing, Michigan.'),\n",
              " KeyDevelopment(year=1913, description=\"Henry Ford began the world's first moving assembly line for cars at the Highland Park Ford Plant.\", evidence=\"This concept was greatly expanded by Henry Ford, beginning in 1913 with the world's first moving assembly line for cars at the Highland Park Ford Plant.\"),\n",
              " KeyDevelopment(year=1914, description=\"An assembly line worker could buy a Model T with four months' pay.\", evidence=\"In 1914, an assembly line worker could buy a Model T with four months' pay.\"),\n",
              " KeyDevelopment(year=1926, description='Fast-drying Duco lacquer was developed, allowing for more car color options.', evidence='Only Japan black would dry fast enough, forcing the company to drop the variety of colours available before 1913, until fast-drying Duco lacquer was developed in 1926.')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geGyFiWMpkML",
        "outputId": "c699948e-b031-42c3-ba1f-ddb66d2642b3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "texts = text_splitter.split_text(document.page_content)\n",
        "vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": 1}\n",
        ")  # Only extract from first document"
      ],
      "metadata": {
        "id": "MDAz9m1LpUmu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_extractor = {\n",
        "    \"text\": retriever | (lambda docs: docs[0].page_content)  # fetch content of top doc\n",
        "} | extractor"
      ],
      "metadata": {
        "id": "2Ncg_TK4pt5E"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = rag_extractor.invoke(\"Key developments associated with cars\")"
      ],
      "metadata": {
        "id": "qL0isVU5p5ES"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key_development in results.key_developments:\n",
        "    print(key_development)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QEd3j_sp6_t",
        "outputId": "b7a7ec57-f82b-4e74-f20b-03028a699b5f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "year=1869 description='Mary Ward became one of the first documented car fatalities in Parsonstown, Ireland.' evidence='Mary Ward became one of the first documented car fatalities in 1869 in Parsonstown, Ireland,'\n",
            "year=1899 description=\"Henry Bliss became one of the US's first pedestrian car casualties in New York City.\" evidence=\"Henry Bliss one of the US's first pedestrian car casualties in 1899 in New York City.\"\n",
            "year=2030 description='Amsterdam plans to ban all fossil fuel vehicles.' evidence='all fossil fuel vehicles will be banned in Amsterdam from 2030.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RkG2GO8MMQz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Handle Files"
      ],
      "metadata": {
        "id": "QeU6HB8tMZ0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get(\"https://en.wikipedia.org/wiki/Car\")\n",
        "data = response.content\n",
        "data[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H9OtMwvMcZd",
        "outputId": "0df1165d-f349-483c-ec02-122c1bf22e83"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'<!DOCTYPE html>\\n<htm'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q magic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML88ViF3M0xn",
        "outputId": "b53e69a9-41ca-4cd1-bb7b-cd646388c5d7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement magic (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for magic\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import magic\n",
        "from langchain.document_loaders.parsers import BS4HTMLParser, PDFMinerParser\n",
        "from langchain.document_loaders.parsers.generic import MimeTypeBasedParser\n",
        "from langchain.document_loaders.parsers.txt import TextParser\n",
        "from langchain_community.document_loaders import Blob\n",
        "\n",
        "# Configure the parsers that you want to use per mime-type!\n",
        "HANDLERS = {\n",
        "    \"application/pdf\": PDFMinerParser(),\n",
        "    \"text/plain\": TextParser(),\n",
        "    \"text/html\": BS4HTMLParser(),\n",
        "}\n",
        "\n",
        "# Instantiate a mimetype based parser with the given parsers\n",
        "MIMETYPE_BASED_PARSER = MimeTypeBasedParser(\n",
        "    handlers=HANDLERS,\n",
        "    fallback_parser=None,\n",
        ")\n",
        "\n",
        "mime = magic.Magic(mime=True)\n",
        "mime_type = mime.from_buffer(data)\n",
        "\n",
        "# A blob represents binary data by either reference (path on file system)\n",
        "# or value (bytes in memory).\n",
        "blob = Blob.from_data(\n",
        "    data=data,\n",
        "    mime_type=mime_type,\n",
        ")\n",
        "\n",
        "parser = HANDLERS[mime_type]\n",
        "documents = parser.parse(blob=blob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "LLGQx9jsMxxo",
        "outputId": "58c1e95b-8f3c-4303-9f2f-d40abd64693e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'magic'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-cf03311ca5e8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmagic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBS4HTMLParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPDFMinerParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMimeTypeBasedParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'magic'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}